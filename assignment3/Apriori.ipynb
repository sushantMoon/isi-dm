{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_sets():\n",
    "    candidate_1 = {}\n",
    "    transactions = 0\n",
    "    dataset = []\n",
    "    for i in range(1, 6, 1):\n",
    "        with open(\"/mnt/Alice/ISI/SEM3/DataMining/Assignments/assignment3/dataset/DataSet{}.txt\".format(i), \"r\") as f:\n",
    "            for line in f:\n",
    "                transac = []\n",
    "                transactions += 1\n",
    "                for word in line.split():\n",
    "                    transac.append(word)\n",
    "                    if word not in candidate_1.keys():\n",
    "                        candidate_1[word] = 1\n",
    "                    else:\n",
    "                        count = candidate_1[word]\n",
    "                        candidate_1[word] = count + 1\n",
    "                dataset.append(transac)\n",
    "    return candidate_1, dataset, transactions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsubsets(S, m):\n",
    "    \"\"\"\n",
    "        Returns all the m element subsets of the set S\n",
    "    \"\"\"\n",
    "    return set(itertools.combinations(S, m))\n",
    "\n",
    "def has_infrequent_subset(candidate_set, frequent_k_1, k):\n",
    "    \"\"\"\n",
    "        Returns True if pruning is required to remove the candidate_set \n",
    "                        if it is not present in the frequent k-1 itemset (frequent_k_1)\n",
    "        else False\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    l = findsubsets(candidate_set, k)\n",
    "    for item in l: \n",
    "        s = []\n",
    "        for l in item:\n",
    "            s.append(l)\n",
    "        s.sort()\n",
    "        if s not in frequent_k_1:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apriori Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_gen(frequent_k_1, k):\n",
    "    \"\"\"\n",
    "        Computes the candidate set (candidate_k) using the frequent k-1 itemset (frequent_k_1) \n",
    "    \"\"\"\n",
    "    length = k\n",
    "    candidate_k = [] \n",
    "    for list1 in frequent_k_1:\n",
    "        for list2 in frequent_k_1:\n",
    "            count = 0\n",
    "            c = []\n",
    "            if list1 != list2:\n",
    "                while count < length-1:\n",
    "                    if list1[count] != list2[count]:\n",
    "                        break\n",
    "                    else:\n",
    "                        count += 1\n",
    "                else:\n",
    "                    if list1[length-1] < list2[length-1]:\n",
    "                        for item in list1:\n",
    "                            c.append(item)\n",
    "                        c.append(list2[length-1])\n",
    "                        if not has_infrequent_subset(c, frequent_k_1, k):\n",
    "                            candidate_k.append(c) \n",
    "                            c = []\n",
    "    return candidate_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_itemsets(frequest_1, dataset):\n",
    "    \"\"\"\n",
    "        Computes all the frequent_itemsets\n",
    "    \"\"\"\n",
    "    k = 2\n",
    "    frequent_k_1 = []\n",
    "    frequent_k = []\n",
    "    frequent_item_sets = []\n",
    "    count = 0\n",
    "    transactions = 0\n",
    "    for item in frequest_1:\n",
    "        frequent_k_1.append(item)\n",
    "    while frequent_k_1 != []:\n",
    "        candidate_k = []\n",
    "        frequent_k = []\n",
    "        candidate_k = apriori_gen(frequent_k_1, k-1)\n",
    "#         print(\"-------------------------CANDIDATE {}-ITEMSET---------------------\".format(k))\n",
    "#         print(\"candidate_{}: {}\".format(k, candidate_k))\n",
    "#         print(\"------------------------------------------------------------------\")\n",
    "        for c in candidate_k:\n",
    "            count = 0\n",
    "            transactions = 0\n",
    "            s = set(c)\n",
    "            for T in dataset:\n",
    "                transactions += 1\n",
    "                t = set(T)\n",
    "                if s.issubset(t) == True:\n",
    "                    count += 1\n",
    "            if (100 * count/transactions) >= support:\n",
    "                c.sort()\n",
    "                frequent_k.append(c)\n",
    "        frequent_k_1 = []\n",
    "#         print(\"-----------------------FREQUENT {}-ITEMSET------------------------\".format(k))\n",
    "#         print(\"frequent_{}: {}\".format(k, frequent_k))\n",
    "#         print(\"------------------------------------------------------------------\")\n",
    "        for l in frequent_k:\n",
    "            frequent_k_1.append(l)\n",
    "        k += 1\n",
    "        if frequent_k != []:\n",
    "            frequent_item_sets.append(frequent_k)\n",
    "    return frequent_item_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_association_rules(frequent_1, dataset):\n",
    "    \"\"\"\n",
    "        Generates and print all the association rules with given support and confidence value\n",
    "    \"\"\"\n",
    "    s = []\n",
    "    r = []\n",
    "    length = 0\n",
    "    count = 1\n",
    "    inc1 = 0\n",
    "    inc2 = 0\n",
    "    num = 1\n",
    "    m = []\n",
    "    L = frequent_itemsets(frequent_1, dataset)\n",
    "    print(\"---------------------ASSOCIATION RULES------------------\")\n",
    "    print(\"RULES \\t SUPPORT \\t CONFIDENCE\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    for list in L:\n",
    "        for l in list:\n",
    "            length = len(l)\n",
    "            count = 1\n",
    "            while count < length: \n",
    "                s = []\n",
    "                r = findsubsets(l,count)\n",
    "                count += 1\n",
    "                for item in r:\n",
    "                    inc1 = 0\n",
    "                    inc2 = 0\n",
    "                    s = []\n",
    "                    m = []\n",
    "                    for i in item:\n",
    "                        s.append(i)\n",
    "                    for T in dataset:\n",
    "                        if set(s).issubset(set(T)) == True:\n",
    "                            inc1 += 1\n",
    "                        if set(l).issubset(set(T)) == True:\n",
    "                            inc2 += 1\n",
    "                    if 100*inc2/inc1 >= confidence:\n",
    "                        for index in l:\n",
    "                            if index not in s:\n",
    "                                m.append(index)\n",
    "                        print(\"Rule#  %d : %s ==> %s %d %d\" %(num, s, m, 100*inc2/len(dataset), 100*inc2/inc1))\n",
    "                        num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Transactions : 100\n",
      "---------------------ASSOCIATION RULES------------------\n",
      "RULES \t SUPPORT \t CONFIDENCE\n",
      "--------------------------------------------------------\n",
      "Rule#  1 : ['TV'] ==> ['cellphone'] 5 71\n",
      "Rule#  2 : ['cellphone'] ==> ['TV'] 5 71\n",
      "Rule#  3 : ['dishwasher'] ==> ['freezer'] 5 71\n",
      "Rule#  4 : ['freezer'] ==> ['dishwasher'] 5 62\n",
      "Rule#  5 : ['dishwasher'] ==> ['juicer'] 5 71\n",
      "Rule#  6 : ['juicer'] ==> ['dishwasher'] 5 55\n",
      "Rule#  7 : ['dishwasher'] ==> ['expresso'] 5 71\n",
      "Rule#  8 : ['expresso'] ==> ['dishwasher'] 5 55\n",
      "Rule#  9 : ['juicer'] ==> ['freezer'] 5 55\n",
      "Rule#  10 : ['freezer'] ==> ['juicer'] 5 62\n",
      "Rule#  11 : ['juicer'] ==> ['microwave'] 5 55\n",
      "Rule#  12 : ['microwave'] ==> ['juicer'] 5 50\n",
      "Rule#  13 : ['expresso'] ==> ['freezer'] 6 66\n",
      "Rule#  14 : ['freezer'] ==> ['expresso'] 6 75\n",
      "Rule#  15 : ['juicer'] ==> ['expresso'] 6 66\n",
      "Rule#  16 : ['expresso'] ==> ['juicer'] 6 66\n",
      "Rule#  17 : ['jewelry'] ==> ['microwave'] 5 55\n",
      "Rule#  18 : ['microwave'] ==> ['jewelry'] 5 50\n",
      "Rule#  19 : ['jewelry'] ==> ['juicer'] 5 55\n",
      "Rule#  20 : ['juicer'] ==> ['jewelry'] 5 55\n",
      "Rule#  21 : ['jewelry'] ==> ['tablet'] 6 66\n",
      "Rule#  22 : ['tablet'] ==> ['jewelry'] 6 66\n",
      "Rule#  23 : ['toys'] ==> ['scooter'] 5 71\n",
      "Rule#  24 : ['scooter'] ==> ['toys'] 5 62\n",
      "Rule#  25 : ['toys'] ==> ['skate'] 5 71\n",
      "Rule#  26 : ['skate'] ==> ['toys'] 5 62\n",
      "Rule#  27 : ['fan'] ==> ['AC'] 5 100\n",
      "Rule#  28 : ['AC'] ==> ['fan'] 5 55\n",
      "Rule#  29 : ['mattress'] ==> ['pillow'] 5 83\n",
      "Rule#  30 : ['pillow'] ==> ['mattress'] 5 83\n",
      "Rule#  31 : ['brush'] ==> ['toothpaste'] 5 100\n",
      "Rule#  32 : ['toothpaste'] ==> ['brush'] 5 62\n",
      "Rule#  33 : ['shampoo'] ==> ['milk'] 5 50\n",
      "Rule#  34 : ['milk'] ==> ['shampoo'] 5 55\n",
      "Rule#  35 : ['cereals'] ==> ['milk'] 6 75\n",
      "Rule#  36 : ['milk'] ==> ['cereals'] 6 66\n",
      "Rule#  37 : ['cereals'] ==> ['honey'] 6 75\n",
      "Rule#  38 : ['honey'] ==> ['cereals'] 6 66\n",
      "Rule#  39 : ['cereals'] ==> ['gel'] 5 62\n",
      "Rule#  40 : ['gel'] ==> ['cereals'] 5 50\n",
      "Rule#  41 : ['cereals'] ==> ['shampoo'] 5 62\n",
      "Rule#  42 : ['shampoo'] ==> ['cereals'] 5 50\n",
      "Rule#  43 : ['honey'] ==> ['milk'] 6 66\n",
      "Rule#  44 : ['milk'] ==> ['honey'] 6 66\n",
      "Rule#  45 : ['bread'] ==> ['milk'] 5 45\n",
      "Rule#  46 : ['milk'] ==> ['bread'] 5 55\n",
      "Rule#  47 : ['bread'] ==> ['honey'] 6 54\n",
      "Rule#  48 : ['honey'] ==> ['bread'] 6 66\n",
      "Rule#  49 : ['bread'] ==> ['butter'] 7 63\n",
      "Rule#  50 : ['butter'] ==> ['bread'] 7 87\n",
      "Rule#  51 : ['bread'] ==> ['cheese'] 7 63\n",
      "Rule#  52 : ['cheese'] ==> ['bread'] 7 70\n",
      "Rule#  53 : ['bread'] ==> ['razor'] 5 45\n",
      "Rule#  54 : ['razor'] ==> ['bread'] 5 71\n",
      "Rule#  55 : ['bread'] ==> ['gel'] 5 45\n",
      "Rule#  56 : ['gel'] ==> ['bread'] 5 50\n",
      "Rule#  57 : ['honey'] ==> ['butter'] 5 55\n",
      "Rule#  58 : ['butter'] ==> ['honey'] 5 62\n",
      "Rule#  59 : ['cheese'] ==> ['butter'] 5 50\n",
      "Rule#  60 : ['butter'] ==> ['cheese'] 5 62\n",
      "Rule#  61 : ['cheese'] ==> ['honey'] 6 60\n",
      "Rule#  62 : ['honey'] ==> ['cheese'] 6 66\n",
      "Rule#  63 : ['gel'] ==> ['milk'] 5 50\n",
      "Rule#  64 : ['milk'] ==> ['gel'] 5 55\n",
      "Rule#  65 : ['razor'] ==> ['gel'] 5 71\n",
      "Rule#  66 : ['gel'] ==> ['razor'] 5 50\n",
      "Rule#  67 : ['shampoo'] ==> ['gel'] 8 80\n",
      "Rule#  68 : ['gel'] ==> ['shampoo'] 8 80\n",
      "Rule#  69 : ['soap'] ==> ['shampoo'] 5 71\n",
      "Rule#  70 : ['shampoo'] ==> ['soap'] 5 50\n",
      "Rule#  71 : ['mouthwash'] ==> ['toothpaste'] 6 85\n",
      "Rule#  72 : ['toothpaste'] ==> ['mouthwash'] 6 75\n",
      "Rule#  73 : ['chocolate'] ==> ['candy'] 5 71\n",
      "Rule#  74 : ['candy'] ==> ['chocolate'] 5 55\n",
      "Rule#  75 : ['luggage'] ==> ['pen'] 8 80\n",
      "Rule#  76 : ['pen'] ==> ['luggage'] 8 72\n",
      "Rule#  77 : ['comforter'] ==> ['pen'] 10 90\n",
      "Rule#  78 : ['pen'] ==> ['comforter'] 10 90\n",
      "Rule#  79 : ['luggage'] ==> ['comforter'] 7 70\n",
      "Rule#  80 : ['comforter'] ==> ['luggage'] 7 63\n",
      "Rule#  81 : ['bag'] ==> ['luggage'] 6 75\n",
      "Rule#  82 : ['luggage'] ==> ['bag'] 6 60\n",
      "Rule#  83 : ['bag'] ==> ['comforter'] 5 62\n",
      "Rule#  84 : ['comforter'] ==> ['bag'] 5 45\n",
      "Rule#  85 : ['bag'] ==> ['cap'] 5 62\n",
      "Rule#  86 : ['cap'] ==> ['bag'] 5 62\n",
      "Rule#  87 : ['pen'] ==> ['cap'] 5 45\n",
      "Rule#  88 : ['cap'] ==> ['pen'] 5 62\n",
      "Rule#  89 : ['comforter'] ==> ['cap'] 6 54\n",
      "Rule#  90 : ['cap'] ==> ['comforter'] 6 75\n",
      "Rule#  91 : ['cereals'] ==> ['milk', 'shampoo'] 5 62\n",
      "Rule#  92 : ['shampoo'] ==> ['cereals', 'milk'] 5 50\n",
      "Rule#  93 : ['milk'] ==> ['cereals', 'shampoo'] 5 55\n",
      "Rule#  94 : ['cereals', 'milk'] ==> ['shampoo'] 5 83\n",
      "Rule#  95 : ['cereals', 'shampoo'] ==> ['milk'] 5 100\n",
      "Rule#  96 : ['milk', 'shampoo'] ==> ['cereals'] 5 100\n",
      "Rule#  97 : ['cereals'] ==> ['honey', 'milk'] 5 62\n",
      "Rule#  98 : ['honey'] ==> ['cereals', 'milk'] 5 55\n",
      "Rule#  99 : ['milk'] ==> ['cereals', 'honey'] 5 55\n",
      "Rule#  100 : ['cereals', 'milk'] ==> ['honey'] 5 83\n",
      "Rule#  101 : ['honey', 'milk'] ==> ['cereals'] 5 83\n",
      "Rule#  102 : ['cereals', 'honey'] ==> ['milk'] 5 83\n",
      "Rule#  103 : ['bread'] ==> ['butter', 'honey'] 5 45\n",
      "Rule#  104 : ['honey'] ==> ['bread', 'butter'] 5 55\n",
      "Rule#  105 : ['butter'] ==> ['bread', 'honey'] 5 62\n",
      "Rule#  106 : ['bread', 'honey'] ==> ['butter'] 5 83\n",
      "Rule#  107 : ['butter', 'honey'] ==> ['bread'] 5 100\n",
      "Rule#  108 : ['bread', 'butter'] ==> ['honey'] 5 71\n",
      "Rule#  109 : ['bread'] ==> ['butter', 'cheese'] 5 45\n",
      "Rule#  110 : ['butter'] ==> ['bread', 'cheese'] 5 62\n",
      "Rule#  111 : ['cheese'] ==> ['bread', 'butter'] 5 50\n",
      "Rule#  112 : ['butter', 'cheese'] ==> ['bread'] 5 100\n",
      "Rule#  113 : ['bread', 'cheese'] ==> ['butter'] 5 71\n",
      "Rule#  114 : ['bread', 'butter'] ==> ['cheese'] 5 71\n",
      "Rule#  115 : ['bread'] ==> ['cheese', 'honey'] 5 45\n",
      "Rule#  116 : ['honey'] ==> ['bread', 'cheese'] 5 55\n",
      "Rule#  117 : ['cheese'] ==> ['bread', 'honey'] 5 50\n",
      "Rule#  118 : ['cheese', 'honey'] ==> ['bread'] 5 83\n",
      "Rule#  119 : ['bread', 'honey'] ==> ['cheese'] 5 83\n",
      "Rule#  120 : ['bread', 'cheese'] ==> ['honey'] 5 71\n",
      "Rule#  121 : ['luggage'] ==> ['comforter', 'pen'] 7 70\n",
      "Rule#  122 : ['comforter'] ==> ['luggage', 'pen'] 7 63\n",
      "Rule#  123 : ['pen'] ==> ['comforter', 'luggage'] 7 63\n",
      "Rule#  124 : ['comforter', 'luggage'] ==> ['pen'] 7 100\n",
      "Rule#  125 : ['luggage', 'pen'] ==> ['comforter'] 7 87\n",
      "Rule#  126 : ['comforter', 'pen'] ==> ['luggage'] 7 70\n",
      "Rule#  127 : ['comforter'] ==> ['cap', 'pen'] 5 45\n",
      "Rule#  128 : ['pen'] ==> ['cap', 'comforter'] 5 45\n",
      "Rule#  129 : ['cap'] ==> ['comforter', 'pen'] 5 62\n",
      "Rule#  130 : ['cap', 'comforter'] ==> ['pen'] 5 83\n",
      "Rule#  131 : ['cap', 'pen'] ==> ['comforter'] 5 100\n",
      "Rule#  132 : ['comforter', 'pen'] ==> ['cap'] 5 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "support = 5\n",
    "confidence = 10\n",
    "\n",
    "candidate_1, dataset, transactions = read_data_sets()\n",
    "print(\"Total Transactions : {}\".format(transactions))\n",
    "frequent_1 = []\n",
    "for key in candidate_1:\n",
    "    if (100 * candidate_1[key]/transactions) >= support:\n",
    "        l = []\n",
    "        l.append(key)\n",
    "        frequent_1.append(l)\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# print(\"-------------------------CANDIDATE 1-ITEMSET---------------------\")\n",
    "# print(\"candidate_1: {}\".format(candidate_1))\n",
    "# print(\"------------------------------------------------------------------\")\n",
    "\n",
    "# print(\"-------------------------FREQUENT 1-ITEMSET---------------------\")\n",
    "# print(\"frequest_1: {}\".format(frequent_1))\n",
    "# print(\"------------------------------------------------------------------\")\n",
    "\n",
    "generate_association_rules(frequent_1, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
